{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA201 - Assignment 4\n",
    "\n",
    "Please use this page http://apps.ecs.vuw.ac.nz/submit/DATA201 for submitssion and submit only this single Jupyter notebook with your code added into it at the appropriate places.\n",
    "\n",
    "The due date is **Saturday 30th May, before midnight**.\n",
    "\n",
    "The dataset for this assignment is file **whitewine.csv** which is provided with this notebook.\n",
    "\n",
    "Please choose menu items *Kernel => Restart & Run All* then *File => Save and Checkpoint* in Jupyter before submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset was adapted from the Wine Quality Dataset (https://archive.ics.uci.edu/ml/datasets/Wine+Quality)\n",
    "\n",
    "### Attribute Information:\n",
    "\n",
    "For more information, read [Cortez et al., 2009: http://dx.doi.org/10.1016/j.dss.2009.05.016].\n",
    "\n",
    "Input variables (based on physicochemical tests):\n",
    "\n",
    "    1 - fixed acidity \n",
    "    2 - volatile acidity \n",
    "    3 - citric acid \n",
    "    4 - residual sugar \n",
    "    5 - chlorides \n",
    "    6 - free sulfur dioxide \n",
    "    7 - total sulfur dioxide \n",
    "    8 - density \n",
    "    9 - pH \n",
    "    10 - sulphates \n",
    "    11 - alcohol \n",
    "Output variable (based on sensory data):\n",
    "\n",
    "    12 - quality (0: normal wine, 1: good wine)\n",
    "    \n",
    "## Problem statement\n",
    "Predict the quality of a wine given its input variables. Use AUC (area under the receiver operating characteristic curve) as the evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load and explore the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "np.random.seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free_sulfur_dioxide  total_sulfur_dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        0  \n",
       "1      9.5        0  \n",
       "2     10.1        0  \n",
       "3      9.9        0  \n",
       "4      9.9        0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"whitewine.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4715 entries, 0 to 4714\n",
      "Data columns (total 12 columns):\n",
      "fixed_acidity           4715 non-null float64\n",
      "volatile_acidity        4715 non-null float64\n",
      "citric_acid             4715 non-null float64\n",
      "residual_sugar          4715 non-null float64\n",
      "chlorides               4715 non-null float64\n",
      "free_sulfur_dioxide     4715 non-null float64\n",
      "total_sulfur_dioxide    4715 non-null float64\n",
      "density                 4715 non-null float64\n",
      "pH                      4715 non-null float64\n",
      "sulphates               4715 non-null float64\n",
      "alcohol                 4715 non-null float64\n",
      "quality                 4715 non-null int64\n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 442.2 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3655\n",
       "1    1060\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"quality\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that this dataset is unbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions and Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[1]. Split the given data using stratify sampling into 2 subsets: training (80%) and test (20%) sets. Use random_state = 42. [1 points]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.2, random_state=42, stratify=data['quality'])\n",
    "\n",
    "X_train = train.drop('quality', axis=1)\n",
    "y_train = train['quality'].copy()\n",
    "\n",
    "X_test = test.drop('quality', axis=1)\n",
    "y_test = test['quality'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2]. Use ``GridSearchCV`` and ``Pipeline`` to tune hyper-parameters for 3 different classifiers including ``KNeighborsClassifier``, ``LogisticRegression`` and ``svm.SVC`` and report the corresponding AUC values on the training and test sets. Note that a scaler may need to be inserted into each pipeline. [6 points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: You may want to use `kernel='rbf'` and tune `C` and `gamma` for `svm.SVC`. Find out how to enable probability estimates (for Question 3).\n",
    "\n",
    "Document: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors best parameters: {'clf__n_neighbors': 45, 'clf__p': 1}\n",
      "K-Nearest Neighbors AUC score(training set): 1.0\n",
      "K-Nearest Neighbors AUC score(test set): 0.9349366337144774\n",
      "K-Nearest Neighbors Confusion Matrix(training set):\n",
      " [[2924    0]\n",
      " [   0  848]]\n",
      "K-Nearest Neighbors Confusion Matrix(test set):\n",
      " [[701  30]\n",
      " [ 66 146]]\n",
      "time: 0.13440759579340616\n",
      "\n",
      "Logistic Regression best parameters: {'clf__C': 100, 'clf__penalty': 'l1'}\n",
      "Logistic Regression AUC score(training set): 0.7867747883488629\n",
      "Logistic Regression AUC score(test set): 0.7987184781767029\n",
      "Logistic Regression Confusion Matrix(training set):\n",
      " [[2754  170]\n",
      " [ 605  243]]\n",
      "Logistic Regression Confusion Matrix(test set):\n",
      " [[690  41]\n",
      " [158  54]]\n",
      "time: 0.03498464822769165\n",
      "\n",
      "SVC best parameters: {'clf__C': 1, 'clf__gamma': 100}\n",
      "SVC AUC score(training set): 0.9991603321890405\n",
      "SVC AUC score(test set): 0.9088480499703171\n",
      "SVC Confusion Matrix(training set):\n",
      " [[2918    6]\n",
      " [  43  805]]\n",
      "SVC Confusion Matrix(test set):\n",
      " [[718  13]\n",
      " [112 100]]\n",
      "time: 0.6369452118873596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "names = [\n",
    "    \"K-Nearest Neighbors\",\n",
    "    \"Logistic Regression\",\n",
    "    \"SVC\" \n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(n_jobs=-1, weights='distance', metric='manhattan'),\n",
    "    LogisticRegression(n_jobs=-1, solver='saga', max_iter = 4000, C=1),\n",
    "    SVC(probability=True, kernel='rbf') # 'linear', 'poly', 'rbf', 'sigmoid'\n",
    "]\n",
    "\n",
    "parameters = [\n",
    "    {\n",
    "        'clf__n_neighbors':list(np.arange(5, 100, 5)), \n",
    "        'clf__p':[1,2]\n",
    "    }, \n",
    "    {\n",
    "        'clf__C':[0.1, 1, 10, 100, 1000],\n",
    "        'clf__penalty':['l1', 'l2']\n",
    "    },\n",
    "    {\n",
    "        'clf__C':[0.1, 1, 10, 100],\n",
    "        'clf__gamma':[0.1, 1, 10, 100]  \n",
    "    }\n",
    "]\n",
    "\n",
    "pipelines = []\n",
    "\n",
    "for name, classifier, params in zip(names, classifiers, parameters):\n",
    "    start = time.time()\n",
    "    \n",
    "    clf_pipe = Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('clf', classifier)\n",
    "    ])\n",
    "    \n",
    "    gs_clf = GridSearchCV(clf_pipe, param_grid=params, n_jobs=-1, cv=5, scoring='roc_auc')\n",
    "    clf_model = gs_clf.fit(X_train, y_train)\n",
    "    train_score = clf_model.score(X_train, y_train)\n",
    "    test_score = clf_model.score(X_test, y_test)\n",
    "    print(\"{} best parameters: {}\".format(name, clf_model.best_params_))\n",
    "    print(\"{} AUC score(training set): {}\".format(name, train_score))\n",
    "    print(\"{} AUC score(test set): {}\".format(name, test_score))\n",
    "    print(\"{} Confusion Matrix(training set):\\n {}\".format(name, confusion_matrix(y_train, clf_model.predict(X_train))))\n",
    "    print(\"{} Confusion Matrix(test set):\\n {}\".format(name, confusion_matrix(y_test, clf_model.predict(X_test))))\n",
    "    pipelines.append((name, clf_model))\n",
    "    \n",
    "    end = time.time()\n",
    "    print(\"time: {}\\n\".format((end-start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[3]. Train a soft ``VotingClassifier`` with the estimators are the three tuned pipelines obtained from [2]. Report the AUC values on the training and test sets. Comment on the performance of the ensemble model. [1 point]**\n",
    "\n",
    "Hint: consider the voting method.\n",
    "\n",
    "Document: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier AUC score(training set): 0.9999903208321503\n",
      "VotingClassifier AUC score(test set): 0.9399956121105748\n",
      "VotingClassifier Confusion Matrix(training set):\n",
      " [[2923    1]\n",
      " [   8  840]]\n",
      "VotingClassifier Confusion Matrix(test set):\n",
      " [[709  22]\n",
      " [ 84 128]]\n",
      "time: 0.691833249727885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "ensemble = VotingClassifier(estimators=pipelines, voting='soft', n_jobs=-1).fit(X_train, y_train)\n",
    "ensemble_train = roc_auc_score(y_train, ensemble.predict_proba(X_train)[:,1], average='macro')\n",
    "ensemble_test = roc_auc_score(y_test, ensemble.predict_proba(X_test)[:,1], average='macro')\n",
    "\n",
    "print(\"VotingClassifier AUC score(training set): {}\".format(ensemble_train))\n",
    "print(\"VotingClassifier AUC score(test set): {}\".format(ensemble_test))\n",
    "print(\"VotingClassifier Confusion Matrix(training set):\\n {}\".format(confusion_matrix(y_train, ensemble.predict(X_train))))\n",
    "print(\"VotingClassifier Confusion Matrix(test set):\\n {}\".format(confusion_matrix(y_test, ensemble.predict(X_test))))\n",
    "\n",
    "end = time.time()\n",
    "print(\"time: {}\\n\".format((end-start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The ensemble model performs marginally better than K-Nearest Neighbors(the difference is 0.005 so might as well be the same performance),  slightly better than SVC and significantly better than logistic regression. The ensemble model doesn't improve on the best performing estimator (KNN) in any meaningful way**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4]. Redo [3] with a sensible set of ``weights`` for the estimators. Comment on the performance of the ensemble model in this case. [1 point]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier best weights: {'weights': [2, 1, 1]}\n",
      "VotingClassifier(weights=[2, 1, 1]) AUC score(training set): 1.0\n",
      "VotingClassifier(weights=[2, 1, 1]) AUC score(test set): 0.9410732261311721\n",
      "VotingClassifier(weights=[2, 1, 1]) Confusion Matrix(training set):\n",
      " [[2924    0]\n",
      " [   0  848]]\n",
      "VotingClassifier(weights=[2, 1, 1]) Confusion Matrix(test set):\n",
      " [[710  21]\n",
      " [ 76 136]]\n",
      "time: 24.4167094151179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "weight_params = []\n",
    "\n",
    "for w1 in range(1,4):\n",
    "    for w2 in range(1,4):\n",
    "        for w3 in range(1,4):\n",
    "            weight_params.append([w1, w2, w3])\n",
    "\n",
    "ensemble_weighted = VotingClassifier(estimators=pipelines, voting='soft', n_jobs=-1)\n",
    "ensemble_gs = GridSearchCV(ensemble_weighted, param_grid={'weights': weight_params}, n_jobs=-1, cv=3, scoring='roc_auc')\n",
    "ensemble_fit = ensemble_gs.fit(X_train, y_train)\n",
    "weighted_train = ensemble_fit.score(X_train, y_train)#roc_auc_score(y_train, ensemble_weighted.predict_proba(X_train)[:,1], average='macro')\n",
    "weighted_test = ensemble_fit.score(X_test, y_test)#roc_auc_score(y_test, ensemble_weighted.predict_proba(X_test)[:,1], average='macro')\n",
    "\n",
    "print(\"VotingClassifier best weights: {}\".format(ensemble_fit.best_params_))\n",
    "print(\"VotingClassifier(weights={}) AUC score(training set): {}\".format(ensemble_fit.best_params_['weights'], weighted_train))\n",
    "print(\"VotingClassifier(weights={}) AUC score(test set): {}\".format(ensemble_fit.best_params_['weights'], weighted_test))\n",
    "print(\"VotingClassifier(weights={}) Confusion Matrix(training set):\\n {}\".format(ensemble_fit.best_params_['weights'], confusion_matrix(y_train, ensemble_fit.predict(X_train))))\n",
    "print(\"VotingClassifier(weights={}) Confusion Matrix(test set):\\n {}\".format(ensemble_fit.best_params_['weights'], confusion_matrix(y_test, ensemble_fit.predict(X_test))))\n",
    "\n",
    "end = time.time()\n",
    "print(\"time: {}\\n\".format((end-start)/60)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN got a perfect 100% accuracy on the training set and the highest AUC score for the test set. It makes it sensible to have a weight of 2 for KNN and 1 for the others. I also tested it out via GridSearchCV and it also gave me 2,1,1 as the best parameters. Giving KNN more voting power gave us a 100% on the training set that we didn't get from the unweighted Voting Classifier. It also gives us a better AUC score for the test set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5]. Use the ``VotingClassifier`` with ``GridSearchCV`` to tune the hyper-parameters of the individual estimators. The parameter grid should be a combination of those in [2]. Report the AUC values on the training and test sets. Comment on the performance of the ensemble model. [1 point]**\n",
    "\n",
    "Note that it may take a long time to run your code for this question.\n",
    "\n",
    "Document: https://scikit-learn.org/stable/modules/ensemble.html#using-the-votingclassifier-with-gridsearchcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier with GridSearchCV best parameters: {'vote__K-Nearest Neighbors__n_neighbors': 70, 'vote__K-Nearest Neighbors__p': 2, 'vote__Logistic Regression__C': 1000, 'vote__Logistic Regression__penalty': 'l1', 'vote__SVC__C': 1, 'vote__SVC__gamma': 100}\n",
      "VotingClassifier with GridSearchCV AUC score(training set): 0.999991127429471\n",
      "VotingClassifier with GridSearchCV AUC score(test set): 0.9399633482177426\n",
      "VotingClassifier with GridSearchCV Confusion Matrix(training set):\n",
      " [[2923    1]\n",
      " [   8  840]]\n",
      "VotingClassifier with GridSearchCV Confusion Matrix(test set):\n",
      " [[715  16]\n",
      " [ 88 124]]\n",
      "time: 107.44066168467204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "params = {}\n",
    "estimators = []\n",
    "for name, classifier, param in zip(names, classifiers, parameters):\n",
    "    estimators.append((name, classifier))\n",
    "    for k in param:\n",
    "        params[k.replace('clf', 'vote__'+name)] = param[k]\n",
    "        \n",
    "vot_ = VotingClassifier(estimators=estimators, voting='soft', n_jobs=-1)\n",
    "pipe=Pipeline(steps=[('scale', scaler), ('vote', vot_)])\n",
    "\n",
    "gs_clf_cv = GridSearchCV(estimator=pipe, param_grid=params, cv=3, n_jobs=-1, scoring='roc_auc')\n",
    "clf_cv = gs_clf_cv.fit(X_train, y_train)\n",
    "cv_train_score = clf_cv.score(X_train, y_train)\n",
    "cv_test_score = clf_cv.score(X_test, y_test)\n",
    "\n",
    "print(\"VotingClassifier with GridSearchCV best parameters: {}\".format(clf_cv.best_params_))\n",
    "print(\"VotingClassifier with GridSearchCV AUC score(training set): {}\".format(cv_train_score))\n",
    "print(\"VotingClassifier with GridSearchCV AUC score(test set): {}\".format(cv_test_score))\n",
    "print(\"VotingClassifier with GridSearchCV Confusion Matrix(training set):\\n {}\".format(confusion_matrix(y_train, clf_cv.predict(X_train))))\n",
    "print(\"VotingClassifier with GridSearchCV Confusion Matrix(test set):\\n {}\".format(confusion_matrix(y_test, clf_cv.predict(X_test))))\n",
    "\n",
    "end = time.time()\n",
    "print(\"time: {}\\n\".format((end-start)/60)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "**Imagine taking 100 minutes to execute and still getting a lower score than the previous two Voting Classifiers. The base Voting Classifier, Voting Classifier with GridSearchCV and SVC all yielded incredibly similiar results whilst the Voting Classifier with estimator weights of 2,1,1 seem to pull ahead by a whopping 0.001 for the test set!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
